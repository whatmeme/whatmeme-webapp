import { NextRequest, NextResponse } from "next/server";
import OpenAI from "openai";

// MCP ì„œë²„ URL
const MCP_SERVER_URL = process.env.MCP_SERVER_URL || "https://sx8ajmutmd.us-east-1.awsapprunner.com/mcp";

// OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const STATIC_MCP_TOOLS = [
  {
    name: "check_meme_status",
    description:
      "ë°ˆì˜ í˜„ì¬ ìœ í–‰/íŠ¸ë Œë”© ìƒíƒœë¥¼ 5ë‹¨ê³„ë¡œ ë‹µí•©ë‹ˆë‹¤\n(ğŸ”¥: 80~100ì  / âš¡: 60~80ì  / âš–ï¸: 40~60ì  / ğŸ§Š: 20~40ì  / â„ï¸: 0~20ì )\n\nì˜ˆì‹œ ì§ˆë¬¸: \"ë§¤ëˆë§¤ëˆí•˜ë‹¤ ë°ˆ í•«í•´?\", \"ê³¨ë°˜ì¶¤ ë°ˆ ìœ í–‰ì´ì•¼?\", \"ìš”ì¦˜ ëŸ­í‚¤ë¹„í‚¤ ë°ˆ ì‹ì—ˆì–´?\"",
    inputSchema: {
      type: "object",
      properties: {
        keyword: {
          type: "string",
          description: "ê²€ìƒ‰í•  ë°ˆ í‚¤ì›Œë“œ ë˜ëŠ” ì§ˆë¬¸",
        },
      },
      required: ["keyword"],
    },
  },
  {
    name: "get_trending_memes",
    description:
      "í˜„ì¬ íŠ¸ë Œë”© TOP 5 ë°ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ ì§ˆë¬¸: \"ìµœì‹  ë°ˆ ì•Œë ¤ì¤˜\", \"ìš”ì¦˜ í•«í•œ ë°ˆ ë­ì•¼?\", \"ì§€ê¸ˆ ìœ í–‰í•˜ëŠ” ë°ˆ ë­ ìˆì–´?\"",
    inputSchema: {
      type: "object",
      properties: {},
    },
  },
  {
    name: "recommend_meme_for_context",
    description:
      "ì£¼ì–´ì§„ ìƒí™©ì— ë§ëŠ” ë°ˆì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ ì§ˆë¬¸: \"ì‹œí—˜ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì„ ë•Œ ë°ˆ\", \"ì‹ ë‚  ë•Œ ì“°ëŠ” ë°ˆ ë­ìˆì–´?\", \"í•©ì˜ ì—†ì´ ê²°ë¡ ì„ ë©‹ëŒ€ë¡œ ì§€ì„ ë•Œ ë°ˆ ì¶”ì²œí•´ì¤˜\"",
    inputSchema: {
      type: "object",
      properties: {
        situation: {
          type: "string",
          description: "ìƒí™© ì„¤ëª…",
        },
      },
      required: ["situation"],
    },
  },
  {
    name: "search_meme_meaning",
    description:
      "ë°ˆì˜ ëœ»/ìœ ë˜/ì‚¬ìš©ì˜ˆì‹œë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ ì§ˆë¬¸: \"ë§¤ëˆë§¤ëˆí•˜ë‹¤ ë°ˆ ì•Œì•„?\", \"ê³¨ë°˜ì¶¤ ë°ˆì´ ë­ì•¼?\", \"ëŸ­í‚¤ë¹„í‚¤ ë°ˆ ì•Œë ¤ì¤˜\"",
    inputSchema: {
      type: "object",
      properties: {
        keyword: {
          type: "string",
          description: "ê²€ìƒ‰í•  ë°ˆ í‚¤ì›Œë“œ ë˜ëŠ” ì§ˆë¬¸",
        },
      },
      required: ["keyword"],
    },
  },
  {
    name: "get_random_meme",
    description:
      "ëœë¤ìœ¼ë¡œ ë°ˆ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì„œ ëœ»/ìœ ë˜/ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\nì˜ˆì‹œ ì§ˆë¬¸: \"ë°ˆ ì•„ë¬´ê±°ë‚˜ ì•Œë ¤ì¤˜\", \"ë°ˆ í•˜ë‚˜ ì¶”ì²œí•´ì¤˜\", \"ë°ˆ ëœë¤ ì¶”ì²œ\"",
    inputSchema: {
      type: "object",
      properties: {},
    },
  },
];

// SSE í˜•ì‹ ì‘ë‹µ íŒŒì‹±
function parseSSEResponse(text: string): any {
  // SSE í˜•ì‹: "event: message\ndata: {...}\n\n"
  const lines = text.split('\n');
  let jsonData = null;
  
  for (const line of lines) {
    if (line.startsWith('data: ')) {
      try {
        jsonData = JSON.parse(line.substring(6)); // "data: " ì œê±°
        break;
      } catch (e) {
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê³„ì† ì‹œë„
      }
    }
  }
  
  return jsonData;
}

// MCP ì„œë²„ì— ìš”ì²­ ë³´ë‚´ê¸°
async function callMCPServer(method: string, params?: any) {
  const mcpRequest: any = {
    jsonrpc: "2.0",
    method: method,
    id: Date.now(),
  };
  
  // paramsê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì¶”ê°€
  if (params && Object.keys(params).length > 0) {
    mcpRequest.params = params;
  }

  // ê°œë°œ í™˜ê²½ì—ì„œë§Œ ìƒì„¸ ë¡œê¹…
  if (process.env.NODE_ENV === "development") {
    console.log(`MCP ìš”ì²­ [${method}]:`, JSON.stringify(mcpRequest, null, 2));
  }

  const response = await fetch(MCP_SERVER_URL, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Accept": "application/json, text/event-stream",
    },
    body: JSON.stringify(mcpRequest),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error(`MCP ì„œë²„ ì˜¤ë¥˜ (${response.status}):`, errorText);
    throw new Error(`MCP ì„œë²„ ì˜¤ë¥˜ (${response.status}): ${errorText}`);
  }

  const contentType = response.headers.get("content-type") || "";
  
  // JSON ì‘ë‹µ ì²˜ë¦¬
  if (contentType.includes("application/json")) {
    const data = await response.json();
    
    // result.contentê°€ SSE í˜•ì‹ì¸ ê²½ìš° íŒŒì‹±
    if (data.result?.content?.[0]?.text) {
      const textContent = data.result.content[0].text;
      if (textContent.includes('event:') && textContent.includes('data:')) {
        const parsedData = parseSSEResponse(textContent);
        if (parsedData) {
          if (process.env.NODE_ENV === "development") {
            console.log(`MCP ì‘ë‹µ [${method}] (SSE íŒŒì‹±ë¨):`, JSON.stringify(parsedData, null, 2));
          }
          return parsedData;
        }
      }
    }
    
    if (process.env.NODE_ENV === "development") {
      console.log(`MCP ì‘ë‹µ [${method}]:`, JSON.stringify(data, null, 2));
    }
    return data;
  }

  // í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬ (SSE í˜•ì‹ì¼ ìˆ˜ ìˆìŒ)
  const text = await response.text();
  
  // SSE í˜•ì‹ì¸ì§€ í™•ì¸
  if (text.includes('event:') && text.includes('data:')) {
    const parsedData = parseSSEResponse(text);
    if (parsedData) {
      if (process.env.NODE_ENV === "development") {
        console.log(`MCP ì‘ë‹µ [${method}] (SSE íŒŒì‹±ë¨):`, JSON.stringify(parsedData, null, 2));
      }
      return parsedData;
    }
  }
  
  // ì¼ë°˜ JSON íŒŒì‹± ì‹œë„
  try {
    const data = JSON.parse(text);
    if (process.env.NODE_ENV === "development") {
      console.log(`MCP ì‘ë‹µ [${method}]:`, JSON.stringify(data, null, 2));
    }
    return data;
  } catch {
    return {
      jsonrpc: "2.0",
      result: {
        content: [{ type: "text", text: text }],
      },
      id: mcpRequest.id,
    };
  }
}

// MCP ë„êµ¬ ëª©ë¡ ê³ ì •ê°’ ì‚¬ìš©
async function getMCPTools() {
  return STATIC_MCP_TOOLS;
}

// MCP ë„êµ¬ë¥¼ OpenAI Tool í˜•ì‹ìœ¼ë¡œ ë³€í™˜
function convertMCPToolToOpenAITool(tool: any) {
  return {
    type: "function",
    function: {
      name: tool.name,
      description: tool.description,
      parameters: tool.inputSchema || {
        type: "object",
        properties: {},
      },
    },
  };
}

// MCP ë„êµ¬ ì‹¤í–‰
async function executeMCPTool(toolName: string, arguments_: any) {
  try {
    const response = await callMCPServer("tools/call", {
      name: toolName,
      arguments: arguments_,
    });

    if (response.error) {
      return `ì˜¤ë¥˜: ${response.error.message}`;
    }

    // MCP ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    if (response.result?.content) {
      const content = response.result.content;
      if (Array.isArray(content)) {
        const textContent = content.find((item: any) => item.type === "text");
        if (textContent) {
          return textContent.text;
        }
        return content[0]?.text || JSON.stringify(content[0]);
      }
      return typeof content === "string" ? content : content.text || JSON.stringify(content);
    }

    return JSON.stringify(response.result);
  } catch (error) {
    return `ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: ${error instanceof Error ? error.message : String(error)}`;
  }
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { messages } = body;

    if (!messages || !Array.isArray(messages)) {
      return NextResponse.json(
        { error: "messages ë°°ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤." },
        { status: 400 }
      );
    }

    // OpenAI API í‚¤ í™•ì¸
    if (!process.env.OPENAI_API_KEY) {
      return NextResponse.json(
        { error: "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤." },
        { status: 500 }
      );
    }

    // MCP ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    const mcpTools = await getMCPTools();
    const tools = mcpTools.map(convertMCPToolToOpenAITool);

    console.log(`ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬: ${tools.length}ê°œ`);

    const systemPrompt = `ë‹¹ì‹ ì€ í•œêµ­ ë°ˆ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
- check_meme_status: ë°ˆì˜ í˜„ì¬ ìœ í–‰ ìƒíƒœ í™•ì¸
- get_trending_memes: í˜„ì¬ íŠ¸ë Œë”© TOP 5 ë°ˆ ëª©ë¡
- recommend_meme_for_context: ìƒí™©ì— ë§ëŠ” ë°ˆ ì¶”ì²œ
- search_meme_meaning: ë°ˆì˜ ëœ»/ìœ ë˜/ì‚¬ìš©ì˜ˆì‹œ ê²€ìƒ‰
- get_random_meme: ëœë¤ ë°ˆ ì¶”ì²œ

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•˜ê³ , í•„ìš”í•˜ë©´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.`;

    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      async start(controller) {
        try {
          const toolCalls: Record<number, { id?: string; name: string; arguments: string }> = {};

          const completionStream = await openai.chat.completions.create({
            model: process.env.OPENAI_MODEL || "gpt-4o-mini",
            messages: [
              { role: "system", content: systemPrompt },
              ...messages.map((msg: any) => ({
                role: msg.role,
                content: msg.content,
              })),
            ],
            tools: tools,
            tool_choice: "auto",
            temperature: 0.7,
            stream: true,
          });

          for await (const chunk of completionStream) {
            const delta = chunk.choices[0]?.delta;
            if (!delta) continue;

            if (delta.tool_calls) {
              for (const toolCall of delta.tool_calls) {
                const index = toolCall.index ?? 0;
                const current = toolCalls[index] || { name: "", arguments: "" };
                toolCalls[index] = current;
                if (toolCall.id) current.id = toolCall.id;
                if (toolCall.function?.name) current.name += toolCall.function.name;
                if (toolCall.function?.arguments) current.arguments += toolCall.function.arguments;
              }
              continue;
            }

            if (typeof delta.content === "string") {
              const payload = JSON.stringify({ type: "delta", content: delta.content });
              controller.enqueue(encoder.encode(`data: ${payload}\n\n`));
            }
          }

          if (Object.keys(toolCalls).length > 0) {
            const firstToolCall = toolCalls[0];
            const functionName = firstToolCall?.name || "";
            let functionArgs: Record<string, any> = {};
            try {
              functionArgs = firstToolCall?.arguments ? JSON.parse(firstToolCall.arguments) : {};
            } catch {
              functionArgs = {};
            }

            console.log(`ë„êµ¬ í˜¸ì¶œ: ${functionName}`, functionArgs);

            const toolResult = await executeMCPTool(functionName, functionArgs);

            const metaPayload = JSON.stringify({
              type: "meta",
              metadata: {
                toolCall: {
                  name: functionName,
                  arguments: functionArgs,
                },
                mcpResponse: toolResult,
              },
            });
            controller.enqueue(encoder.encode(`data: ${metaPayload}\n\n`));

            const toolCallId = firstToolCall?.id || "tool_call_0";
            const finalStream = await openai.chat.completions.create({
              model: process.env.OPENAI_MODEL || "gpt-4o-mini",
              messages: [
                {
                  role: "system",
                  content: "ë‹¹ì‹ ì€ í•œêµ­ ë°ˆ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
                },
                ...messages.map((msg: any) => ({
                  role: msg.role,
                  content: msg.content,
                })),
                {
                  role: "assistant",
                  content: null,
                  tool_calls: [
                    {
                      id: toolCallId,
                      type: "function",
                      function: {
                        name: functionName,
                        arguments: JSON.stringify(functionArgs),
                      },
                    },
                  ],
                },
                {
                  role: "tool",
                  tool_call_id: toolCallId,
                  content: toolResult,
                },
              ],
              tools: tools,
              tool_choice: "none",
              temperature: 0.7,
              stream: true,
            });

            for await (const finalChunk of finalStream) {
              const finalDelta = finalChunk.choices[0]?.delta;
              if (finalDelta?.content) {
                const payload = JSON.stringify({ type: "delta", content: finalDelta.content });
                controller.enqueue(encoder.encode(`data: ${payload}\n\n`));
              }
            }
          }

          controller.enqueue(encoder.encode(`data: {"type":"done"}\n\n`));
          controller.close();
        } catch (error) {
          const errorPayload = JSON.stringify({
            type: "error",
            error: error instanceof Error ? error.message : "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
          });
          controller.enqueue(encoder.encode(`data: ${errorPayload}\n\n`));
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "text/event-stream; charset=utf-8",
        "Cache-Control": "no-cache, no-transform",
        Connection: "keep-alive",
      },
    });
  } catch (error: any) {
    console.error("Chat API ì˜¤ë¥˜:", error);
    
    // OpenAI ì¿¼í„° ì˜¤ë¥˜ ì²˜ë¦¬
    if (error?.status === 429 || error?.code === "insufficient_quota") {
      return NextResponse.json(
        {
          error: "OpenAI API ì¿¼í„°ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ê³„ì •ì˜ ê²°ì œ ì •ë³´ì™€ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
          details: error.message,
        },
        { status: 429 }
      );
    }
    
    // OpenAI ì¸ì¦ ì˜¤ë¥˜
    if (error?.status === 401) {
      return NextResponse.json(
        {
          error: "OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. .env.local íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
        },
        { status: 401 }
      );
    }
    
    return NextResponse.json(
      {
        error: error instanceof Error ? error.message : "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        details: error?.message,
      },
      { status: 500 }
    );
  }
}
